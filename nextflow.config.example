/**
 * ===========================
 * GEMmaker Configuration File
 * ===========================
 *
 * This file provides the configuration settings for the GEMmaker workflow.
 */
manifest {
  mainScript = "main.nf"
  defaultBranch = "master"
  nextflowVersion = ">=0.32.0"
}

/**
 * Parameters
 */
params {

  /**
   * Input
   *
   * Parameters for input files and directories.
   */
  input {
    //
    // The path (full or relative) to the list of fastq_ids to be downloaded
    // from NCBI. This must be a text file with one SRR/DRR/ERR number per line.
    // No blank lines are allowed
    // If no remote files are to be downloaded, set parameter as "none"
    //
    remote_list_path = "none"

    //
    // The glob that retrieves locally stored "fastq" files
    // An example of a proper glob to retreive files can be seen below as the
    // default. This glob pattern will find all files that have an ending of
    // "_1.fastq" or "_2.fastq" in the subdirectories of the folder "Sample".
    // If no local files are to be used, set parameter as "none"
    //
    local_samples_path = "${PWD}/examples/LocalRunExample/Data/Sample*/*_{1,2}.fastq"

    //
    // The full file system path of the directory containing the genome reference
    // files. The reference genome is provided to this workflow via a set of files
    // in a single directory. The list of files includes:
    //
    // 1) A FASTA file containing the genomic sequence
    // 2) Hisat2 index files for the reference, which must be created with bisat2-bulid
    // 3) A GTF file containing the genes annotated within hte genome
    //
    // All files for the reference genome must begin with the same file prefix. For
    // example, if the prefix is TAIR10_Araport11 then the following files should be
    // present all with "TAIR10_Araport11" as the file prefix:
    // TAIR10_Araport11.fna, TAIR10_Araport11.1.ht2, TAIR10_Araport11.2.ht2, (with
    // potentially more hisat2 index files), and TAIR10_Araport11.gtf.
    //
    reference_path = "${PWD}/examples/LocalRunExample/reference/"

    //
    // The prefix (used by hisat2-build) for the genome reference files. Note:
    // all files in the reference directory must have this prefix as well.
    //
    reference_prefix = "CORG"
  }

  /**
   * Output
   *
   * Parameters for output files and directories.
   */
  output {
    //
    // Results generated by this workflow are stored in directories that use
    // "sample_id". as directory name. If the "fastq_run_id" is not associated
    // with a "sample_id" (for example, with local files), then a "sample_id"
    // will be automatically assigned by adding "Sample_" to the begining of the
    // "fastq_run_id" (for example, "123_file1_1.fastq" would be assigned the
    // sample_id "Sample_123_file1_1"). The default storage pattern is to make
    // one directory for each "sample_id", with the parameter set as:
    //
    //    outputdir_sample_id = { "${PWD}/${sample_id}" }
    //
    // However, if you have a large amount of samples (typically 1000 +), it may
    // be problematic to have hundreds or thousands of sample directories in
    // one place. To fix this you can assign a glob pattern to organize the
    // results into a cascading file system. For example, the following:
    //
    //    outputdir_sample_id = { "${PWD}/${sample_id[0..2]}/${sample_id[3..4]}/${sample_id[5..6]}/${sample_id.drop(7)}/${sample_id}" }
    //
    // Will organize files downloaded from NCBI in a nesting fashion. The
    // output of the sample_id "SRX0123456" would be put in the directory
    // "/SRX/12/34/56/SRX123456/". You can modify the above glob patterns for
    // your needs.
    //
    outputdir_sample_id = { "${PWD}/${sample_id}" }

    //
    // publish mode for publishDir
    //
    // Options are the standard nextflow stage options. These are:
    // "link"     Reccomended, this creates a hardlink of all files that should be
    //            published
    // "symlink"  Use when hardlink is not possible.
    // "copy"     Not recommended. This copies all files to desired publshdir
    //            after they are created in the pipeline. Takes alot of memory and
    //            will slow down pipeline drastically
    //
    publish_mode = "link"
  }

  /**
   * Execution
   *
   * Parameters for the executor, primarily for non-local executors.
   */
  execution {
    //
    //  Maximum number of processes to execute at once
    //
    queue_size = 100

    //
    //  Number of threads for multi-threaded processes
    //
    threads = 1

    //
    //  Number of times to resubmit a failed process before terminating
    //  the entire workflow
    //
    max_retries = 2
  }

  /**
   * Software
   *
   * Parameters specific to each process in the workflow.
   */
  software {
    /**
     * fastq_dump
     *
     * retreives fastq files from NCBI
     */
    fastq_dump {
      //
      //  Time allowed for fastq_dump
      //
      time = "48h"
    }

    /**
     * fastqc_1
     *
     * FASTQC is a software that generates reports on read data. It looks at
     * quality, GC content, and much more. This nextflow pipeline runs fastQC
     * two times, once before Trimmomatic, and once after.
     */
    fastqc_1 {
      //
      //  Time allowed for fastqc_1 (fastqc prior to trimmomatic)
      //
      time = "24h"
    }

    /**
     * Parameters for the trimmomatic software.
     *
     * Trimmomatic is used to clean low quality ends and adapters
     * from sequence data.
     */
    trimmomatic {
      //
      //  Time allowed for trimmomatic
      //
      time = "72h"

      //
      // The location of the trimmomatic clipping files.
      //
      clip_path = "${PWD}/files/fasta_adapter.txt"

      //
      // The minimum read length to be used taken as a percentage of the mean of
      // each sample. This is different than how Trimmomatic normally does this
      // step. This is so that many samples of different length reads can be
      // processed at the same time.
      //
      MINLEN = "0.7"

      //
      // Quality score, can be "-phred33", "-phred64" or empty "".
      //
      // Leaving this option empty will cause different behavior depending on
      // your version of Trimmomatic. Prior to v0.32, phred64 is used by default.
      // Since v0.32, the quality encoding is determined automatically if left blank.
      //
      // The recommended method is to use trimmomatic v0.32 or newer and leave this
      // option blank, in case you have samples with both types of encoding.
      //
      quality = ""

      //
      // Trimmomatic options for how to process each read. See documentation at:
      // http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf
      //
      SLIDINGWINDOW = "4:15"
      LEADING = "3"
      TRAILING = "6"
    }

    /**
     * fastqc_2
     *
     * FASTQC is a software that generates reports on read data. It looks at
     * quality, GC content, and much more. This nextflow pipeline runs fastQC
     * two times, once before Trimmomatic, and once after.
     */
    fastqc_2 {
      //
      //  Time allowed for fastqc_2 (fastqc after trimmomatic)
      //
      time = "48h"
    }

    /**
     * hisat2
     *
     * Aligns reads to the reference genome.
     */
    hisat2 {
      //
      //  Time allowed for hisat2
      //
      time = "48h"
    }

    /**
     * samtools_sort
     *
     * Sorts the SAM alignment file from hisat2 and converts it to a binary BAM
     * file
     */
    samtools_sort {
      //
      //  Time allowed for samtools_sort
      //
      time = "48h"
    }

    /**
     * samtools_index
     *
     * Indexes the BAM alignment file
     */
    samtools_index {
      //
      //  Time allowed for samtools_index
      //
      time = "12h"
    }

    /**
     * stringtie
     *
     * Generates expression level transcript abundance.
     */
    stringtie {
      //
      //  Time allowed for stringtie
      //
      time = "12h"
    }

    /**
     * fpkm_or_tpm
     *
     * Creates fpkm and/or tpm file from stringtie *.ga
     */
    fpkm_or_tpm {
      //
      //  Should this workflow output fpkm and/or tpm?
      //
      fpkm = true
      tpm = true
    }
  }
}



/**
 * Trace
 *
 * The following instructs nextflow to collect performance data. You can use
 * this data to generate trace reports. See the Tracing and Visualization section
 * of the Nextflow Documentation for more information.
 *
 * Most likely you do not need to change these settings unless you are familiar
 * with how Nextflow collects trace data and you specifically want to tweak it.
 */
trace {
  fields = "task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes"
  raw = true
}



/**
 * Nextflow can be executed on any number of high-performance computing
 * infrastructures.  The profile section allows you to customize execution
 * of this workflow for any number of systems.  Please see the Nextflow
 * documentation for a complete list of settings.  Here we provide a few
 * critical settings as well as some examples.
 */
profiles {

  //
  // The standard profile is for use on a local machine. It will run
  // each process as a literal process on the machine.
  //
  standard {
    process.executor = "local"
  }

  //
  // Clemson's Palmetto cluster uses the PBS scheduler. Here we provide
  // an example for execution of this workflow on Palmetto with some
  // defaults for all steps of the workflow.
  //
  // Since the PBS resource list does not support disk or local scratch,
  // we must use a custom resource list in order to guarantee a certain
  // amount of local scratch space. In this case, we select nodes from phases
  // on Palmetto which have plenty of local scratch and tend to be highly
  // available. Additionally, nextflow does not properly support the "ncpus"
  // and "mem" options in PBS Professional, so we must specify these resources
  // using "clusterOptions" instead of the "cpus" and "mem" directives.
  //
  pbs {
    process {
      executor = "pbs"
      time = "8h"
      errorStrategy = "retry"
      maxRetries = "${params.execution.max_retries}"
      scratch = true
      stageInMode = "copy"

      withLabel: "multithreaded" {
        clusterOptions = "-l select=1:phase=4:mem=8gb:ncpus=${params.execution.threads}"
      }
      withLabel: "!multithreaded" {
        clusterOptions = "-l select=1:phase=4:mem=2gb:ncpus=2"
      }
    }
    executor {
      queueSize = "${params.execution.queue_size}"
    }
  }

  //
  // WSU's Kamiak cluster uses the SLURM scheduler. Here we provide
  // an example for execution of this workflow on Kamiak with some
  // defaults for all steps of the workflow.
  //
  slurm {
    process {
      executor = "slurm"
      queue = {users queue}
      time = "4h"
      errorStrategy = "retry"
      maxRetries = "${params.execution.max_retries}"

      withLabel: "multithreaded" {
        cpus = ${params.execution.threads}
      }
      withLabel: "!multithreaded" {
        cpus = 1
      }
    }
    executor {
      queueSize = "${params.execution.queue_size}"
    }
  }
}
