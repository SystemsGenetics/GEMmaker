/**
 * ==================
 * GEMmaker Configuration File
 * ==================
 *
 * This file provides the configuration settings needed to customize execution
 * of this workflow. A description of each setting is provided.
 *
 */
params {

  /**
   * Input
   * Information about how GEMmaker should access raw files
   *
   */
  input {
    //
    // The path (full or relative) to the list of fastq_ids to be downloaded
    // from NCBI. This must be a text file with one SRR/DRR/ERR number per line.
    // No blank lines are allowed
    // If no remote files are to be downloaded, set parameter as "none"
    //
    remote_list_path = "none"

    //
    // The glob that retrieves locally stored "fastq" files
    // An example of a proper glob to retreive files can be seen below as the
    // default. This glob pattern will find all files that have an ending of
    // "_1.fastq" or "_2.fastq" in the subdirectories of the folder "Sample".
    // If no local files are to be used, set parameter as "none"
    //
    local_samples_path = "${PWD}/examples/LocalRunExample/Data/Sample*/*_{1,2}.fastq"
  }

  /**
   * Output
   * How GEMmaker outputs files and creates directories. You can also indicate if
   * it should collect performance data.
   *
   */
  output {
    //
    // Results generated by this workflow are stored in directories that use
    // "sample_id". as directory name. If the "fastq_run_id" is not associated
    // with a "sample_id" (for example, with local files), then a "sample_id"
    // will be automatically assigned by adding "Sample_" to the begining of the
    // "fastq_run_id" (for example, "123_file1_1.fastq" would be assigned the
    // sample_id "Sample_123_file1_1"). The default storage pattern is to make
    // one directory for each "sample_id", with the parameter set as:
    //
    //    outputdir_sample_id = { "${PWD}/${sample_id}" }
    //
    // However, if you have a large amount of samples (typically 1000 +), it may
    // be problematic to have hundreds or thousands of sample directories in
    // one place. To fix this you can assign a glob pattern to organize the
    // results into a cascading file system. For example, the following:
    //
    //    outputdir_sample_id = { "${PWD}/${sample_id[0..2]}/${sample_id[3..4]}/${sample_id[5..6]}/${sample_id.drop(7)}/${sample_id}" }
    //
    // Will organize files downloaded from NCBI in a nesting fashion. The
    // output of the sample_id "SRX0123456" would be put in the directory
    // "/SRX/12/34/56/SRX123456/". You can modify the above glob patterns for
    // your needs.
    //
    outputdir_sample_id = { "${PWD}/${sample_id}" }

    //
    // staging_mode for publishDir and stageInMode
    //
    // Options are the standard nextflow stage options. These are:
    // "link"     Reccomended, this creates a hardlink of all files that should be
    //            published
    // "symlink"  Use when hardlink is not possible.
    // "copy"     Not recommended. This copies all files to desired publshdir
    //            after they are created in the pipeline. Takes alot of memory and
    //            will slow down pipeline drastically
    //
    staging_mode = "link"
  }

  /**
   * Software Params
   *
   * This section references each process in the GEMmaker nextflow. This can be
   * used to modify how each Software within GEMmaker will act.
   */
  software {
    /**
     * fastq_dump
     *
     * retreives fastq files from NCBI
     */
    fastq_dump {
      //
      //  Time allowed for fastq_dump
      //
      time = "48h"
    }

    /**
     * fastqc_1
     *
     * FASTQC is a software that generates reports on read data. It looks at
     * quality, GC content, and much more. This nextflow pipeline runs fastQC
     * two times, once before Trimmomatic, and once after.
     */
    fastqc_1 {
      //
      //  Time allowed for fastqc_1 (fastqc prior to trimmomatic)
      //
      time = "24h"
    }

    /**
     * Parameters for the trimmomatic software.
     *
     * Trimmomatic is used to clean low quality ends and adapters
     * from sequence data.
     */
    trimmomatic {
      //
      //  Time allowed for trimmomatic
      //
      time = "72h"

      //
      //  Number of threads for trimmomatic
      //
      threads = 1

      //
      // The location of the trimmomatic clipping files.
      //
      clip_path = "${PWD}/files/fasta_adapter.txt"

      //
      // The minimum read length to be used taken as a percentage of the mean of
      // each sample. This is different than how Trimmomatic normally does this
      // step. This is so that many samples of different length reads can be
      // processed at the same time.
      //
      MINLEN = "0.7"

      // Quality encoding. Choose "-phred33", "phred64" or leave blank ""
      // (If left blank, Trimmomatic will determine which to use automatically.
      // (Trimmomatic version 0.32 and on ))
      // Reccomended to leave blank unless there is a specific reason not to.
      Quality = ""

      //
      // Trimmomatic options for how to process each read. See documentation at:
      // http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf
      //
      SLIDINGWINDOW = "4:15"
      LEADING = "3"
      TRAILING = "6"
    }

    /**
     * fastqc_2
     *
     * FASTQC is a software that generates reports on read data. It looks at
     * quality, GC content, and much more. This nextflow pipeline runs fastQC
     * two times, once before Trimmomatic, and once after.
     */
    fastqc_2 {
      //
      //  Time allowed for fastqc_2 (fastqc after trimmomatic)
      //
      time = "48h"
    }

    /**
     * hisat2
     *
     * Aligns reads to the reference genome.
     */
    hisat2 {
      //
      //  Time allowed for hisat2
      //
      time = "48h"

      //
      //  Number of threads for hisat2
      //
      threads = 1

      /**
       * Parameters for the reference genome for hisat2.
       *
       * The reference genome is provided to this workflow via a set of files housed in a
       * single directory.  The list of files includes:
       *
       * 1) A FASTA file containing the genomic sequence.
       * 2) Hisat2 index files for the reference.  These index files must
       *    be created using the hisat2-build function.
       * 3) A GTF file containing the genes annotated within hte genome.
       *
       * All files for the reference genome must begin with the same file prefix. For
       * example, if the prefix is TAIR10_Araport11 then the following files should be
       * present all with "TAIR10_Araport11" as the file prefix:
       * TAIR10_Araport11.fna, TAIR10_Araport11.1.ht2, TAIR10_Araport11.2.ht2, (with
       * potentially more hista2 index files), and TAIR10_Araport11.gtf.
       */

      //
      // The full file system path of the directory containing the
      // genome reference files.
      //
      path = "${PWD}/examples/LocalRunExample/reference/"

      //
      // The prefix (used by hisat2-build) for the genome reference files. Note:
      // all files in the reference directory must have this prefix as well.
      //
      prefix = "CORG"
    }

    /**
     * samtools_sort
     *
     * Sorts the SAM alignment file from hisat2 and converts it to a binary BAM
     * file
     */
    samtools_sort {
      //
      //  Time allowed for samtools_sort
      //
      time = "48h"
    }

    /**
     * samtools_index
     *
     * Indexes the BAM alignment file
     */
    samtools_index {
      //
      //  Time allowed for samtools_index
      //
      time = "10h"
    }

    /**
     * stringtie
     *
     * Generates expression level transcript abundance.
     */
    stringtie {
      //
      //  Time allowed for stringtie
      //
      time = "10h"

      //
      //  Number of threads for stringtie
      //
      threads = 1
    }

    /**
     * fpkm_or_tpm
     *
     * Creates either fpkm or tpm file from stringtie *.ga
     */
    fpkm_or_tpm {
      //
      //  Should this workflow output fpkm and/or tpm?
      //
      fpkm = true
      tpm = true
    }
  }
}



/**
 * Trace
 * The following instructs nextflow to collect performance data. You can use
 * this data to generate trace reports. See the Tracing and Visualization section
 * of the Nextflow Documentation for more information.
 *
 * Most likely you do not need to change these settings unless you are familiar
 * with how Nextflow collects trace data and you specifically want to tweak it.
 */
trace {
  fields = "task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes"
  raw = true
}



/**
 * Nextflow can be executed on any number of high-performance computing
 * infrastructures.  The profile section allows you to customize execution
 * of this workflow for any number of systems.  Please see the Nextflow
 * documentation for a complete list of settings.  Here we provide a few
 * critical settings as well as some examples.
 */
profiles {

  //
  // Indicates the executor plugin. By default this is set to use a local
  // desktop computer.
  //
  standard {
    process.executor = "local"
  }

  //
  // Clemson's Palmetto cluster uses the PBS scheduler. Here we provide
  // an example for execution of this workflow on Palmetto with some
  // defaults for all steps of the workflow
  //
  pbs {
    process {
      executor = "pbs"
      cpus = 1
      time = "4h"
      maxRetries = 2
      errorStrategy = "retry"
    }
    executor {
      queueSize = 1000
    }
  }

  //
  // WSU's Kamiak cluster uses the SLURM scheduler. Here we provide
  // an example for execution of this workflow on Kamiak with some
  // defaults for all steps of the workflow
  //
  slurm {
    process {
      executor = "slurm"
      queue = {users queue}
      cpus = 1
      time = "4h"
      maxRetries = 2
      errorStrategy = "retry"
    }
    executor {
      queueSize = 100
    }
  }
}
