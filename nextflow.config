/**
* ==================
* GEMaker Paramaters
* ==================
*
* This file provides the configuration settings needed to customize execution
* of this workflow. A description of each setting is provided.
*
*/
params {

  /**
  * Input
  * Information about how GEMaker should access raw files
  *
  */
  input {
    //
    // The path (full or relative) to the list of fastq_ids to be downloaded
    // from NCBI. This must be a text file with one SRR/DRR/ERR number per line.
    // No blank lines are allowed
    // If no remote files are to be downloaded, set parameter as "none"
    //
    remote_list_path = "none"

    //
    // The glob that retrieves locally stored "fastq" files
    // An example of a proper glob to retreive files can be seen below as the
    // default. This glob pattern will find all files that have an ending of
    // "_1.fastq" or "_2.fastq" in the subdirectories of the folder "Sample".
    // If no local files are to be used, set parameter as "none"
    //
    local_samples_path = "${PWD}/examples/Data/Sample*/*_{1,2}.fastq"
  }

  /**
  * Output
  * How GEMaker outputs files and creates directories. You can also indicate if
  * it should collect performance data.
  *
  */
  output {
    //
    // Results generated by this workflow are stored in directories that use
    // "sample_id". as directory name. If the "fastq_run_id" is not associated
    // with a "sample_id" (for example, with local files), then a "sample_id"
    // will be automatically assigned by adding "Sample_" to the begining of the
    // "fastq_run_id" (for example, "123_file1_1.fastq" would be assigned the
    // sample_id "Sample_123_file1_1"). The default storage pattern is to make
    // one directory for each "sample_id", with the parameter set as:
    //
    //    outputdir_sample_id = { "${PWD}/${sample_id}" }
    //
    // However, if you have a large amount of samples (typically 1000 +), it may
    // be problematic to have hundreds or thousands of sample directories in
    // one place. To fix this you can assign a glob pattern to organize the
    // results into a cascading file system. For example, the following:
    //
    //    outputdir_sample_id = { "${PWD}/${sample_id[0..2]}/${sample_id[3..4]}/${sample_id[5..6]}/${sample_id.drop(7)}/${sample_id}" }
    //
    // Will organize files downloaded from NCBI in a nesting fashion. The
    // output of the sample_id "SRX0123456" would be put in the directory
    // "/SRX/12/34/56/SRX123456/". You can modify the above glob patterns for
    // your needs.
    //
    outputdir_sample_id = { "${PWD}/${sample_id}" }

    //
    // staging_mode for publishDir and stageInMode
    //
    // Options are the standard nextflow stage options. These are:
    // "link"     Reccomended, this creates a hardlink of all files that should be
    //            published
    // "symlink"  Use when hardlink is not possible.
    // "copy"     Not recommended. This copies all files to desired publshdir
    //            after they are created in the pipeline. Takes alot of memory and
    //            will slow down pipeline drastically
    //
    staging_mode = "link"

  }

  /**
  * Trace
  * The following instructs nextflow to collect performance data. You can use
  * this data to generate trace reports. See the Tracing and Visualization section
  * of the Nextflow Documentation for more information.
  *
  * Most likely you do not need to change these settings unless you are familiar
  * with how Nextflow collects trace data and you specifically want to tweak it.
  */
  trace {
    enabled = true
    file = 'pipeline_trace.txt'
    fields = 'task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes'
  }

  /**
  * Parameters for the reference genome.
  *
  * The reference genome is provided to this workflow via a set of files housed in a
  * single directory.  The list of files includes:
  *
  * 1) A FASTA file containing the genomic sequence.
  * 2) Hisat2 index files for the reference.  These index files must
  *    be created using the hisat2-build function.
  * 3) A GTF file containing the genes annotated within hte genome.
  *
  * All files for the reference genome must begin with the same file prefix. For
  * example, if the prefix is TAIR10_Araport11 then the following files should be
  * present all with "TAIR10_Araport11" as the file prefix:
  * TAIR10_Araport11.fna, TAIR10_Araport11.1.ht2, TAIR10_Araport11.2.ht2, (with
  * potentially more hista2 index files), and TAIR10_Araport11.gtf.
  */
  ref {
    //
    // The full file system path of the directory containing the
    // genome reference files.
    //
    path = "${PWD}/examples/reference/"

    //
    // The prefix (used by hisat2-build) for the genome reference files. Note:
    // all files in the reference directory must have this prefix as well.
    //
    prefix = "CORG"
  }


  /**
  * Parameters for the trimmomatic software.
  *
  * Trimmomatic is used to clean low quality ends and adapters
  * from sequence data.
  */
  trimmomatic {
    //
    // The location of the trimmomatic clipping files.
    //
    clip_path = $ILLUMINACLIP_PATH

    //
    // The minimum read length to be used taken as a percentage of the mean of
    // each sample.
    //
    MINLEN = ".7"
  }
}



/**
* Nextflow can be executed on any number of high-performance computing
* infrastructures.  The profile section allows you to customize execution
* of this workflow for any number of systems.  Please see the Nextflow
* documentation for a complete list of settings.  Here we provide a few
* critical settings as well as some examples.
*/
profiles {

  standard {
  //
  // Indicates the executor plugin. By default this is set to use a local
  // desktop computer.
  //
  process.executor = 'local'

  }

  //
  // WSU's Kamiak cluster uses the SLURM scheduler. Here we provide
  // an example for execution of this workflow on Kamiak with some
  // defaults for all steps of the workflow
  //
  slurm_cluster {
    process {
    executor = 'slurm'
    queue = {users queue}
    cpus = 1
    time = '4h'
    maxRetries = 2
    errorStrategy = 'retry'
    }
    executor {
    queueSize = 100
    }
  }
}
