/**
 * ==================
 * GEMaker Paramaters
 * ==================
 *
 * This file provides the configuration settings needed to customize execution
 * of this workflow. A description of each setting is provided.
 *
 */
params {

  /**
   * Fastq file storage information information
   *
   */
   fatsq {

    /**
     * The path (full or relative) to the list of fastq_ids to be downloaded
     * from NCBI. This must be a text file with one SRR/DRR/ERR number per line.
     * If no remote files are to be downloaded, set parameter as "none"
     */
    remote_list_path = 'none'

    /**
     * The glob that retrieves locally stored "fastq" files
     *
     * An example of a proper glob to retreive all files with the extension
     * ".fastq" that are contained within sub directories of the imaginary
     * directory "Imagine_Dir_ would be:
     */
     //  "${PWD}/Imagine_Dir/*/*_{1,2}.fastq"
    /**
     * If no local files are to be used, set parameter as "none"
     */
    local_samples_path = "${PWD}/examples/Data/Sample*/*_{1,2}.fastq"

    /**
     *
     *
     *
     *
     */
    /** JOHN WILL REWRITE THIS
     * Results generated by this workflow are stored in directories that use the
     * sample_id.  Thus, all results for a single sample are contained in its
     * respsective directory.  For SRA files the IDs provided are run_ids (e.g.
     * with SRR, DRR and ERR prefixed numbers), but end-results are stored in
     * folders with their respective sample_id (e.g. SRX prefix). The
     * experiment IDs are threfore the sample names.  There are two  parameters so
     * that a file system can be used when there are many fastq files (typically
     * 500 plus files). If you are running this on less than this, we suggest that
     * you use the following parameters:
     *
     *    outputdir_sample_id = { "${PWD}/${sample_id}" }
     *
     * If you have a large amount of samples, it may be problematic to have hundreds or
     * thousands of sample directories in once place. Therefore, you can use the
     * following organize results into a cascading file system. For example, "SRX0123456"
     * would have its files in the directory "/sample_id/12/34/56/SRR123456/". The following
     * will support this type of output structure.
     *
     *    outputdir_sample_id = { "${PWD}/${sample_id[0..2]}/${sample_id[3..4]}/${sample_id[5..6]}/${sample_id.drop(7)}/${sample_id}" }
     */
    outputdir_sample_id = { "${PWD}/${sample_id}" }

    /**
     * staging_mode for publishDir and stageInMode
     *
     * Options are the standard nextflow stage options. These are:
     * "link"     Reccomended, this creates a hardlink of all files that should be
     *            published
     * "symlink"  Use when hardlink is not possible.
     * "copy"     Not recommended. This copies all files to desired publshdir
     *            after they are created in the pipeline. Takes alot of memory and
     *            will slow down pipeline drastically
     */
    staging_mode = 'link'

  /**
   * Parameters for the reference genome.
   *
   * The reference genome is provided to this workflow via a set of files housed in a
   * single directory.  The list of files includes:
   *
   * 1) A FASTA file containing the genomic sequence.
   * 2) Hisat2 index files for the reference.  These index files must
   *    be created using the hisat2-build function.
   * 3) A GTF file containing the genes annotated within hte genome.
   *
   * All files for the reference genome must begin with the same file prefix. For
   * example, if the prefix is TAIR10_Araport11 then the following files should be
   * present all with "TAIR10_Araport11" as the file prefix:
   * TAIR10_Araport11.fna, TAIR10_Araport11.1.ht2, TAIR10_Araport11.2.ht2, (with
   * potentially more hista2 index files), and TAIR10_Araport11.gtf.
   */
  ref {

    /**
     * The full file system path of the directory containing the
     * genome reference files.
     */
    path = "${PWD}/examples/reference/"

    /**
     * The prefix (used by hisat2-build) for the genome reference files. Note:
     * all files in the reference directory must have this prefix as well.
     */
    prefix = "CORG"
  }


  /**
   * Parameters for the trimmomatic software.
   *
   * Trimmomatic is used to clean low quality ends and adapters
   * from sequence data.
   */
  trimmomatic {

    /**
     * The location of the trimmomatic clipping files.
     */
    clip_path = "/data/ficklin/software/Trimmomatic-0.36/adapters"

    /**
     * The minimum read length to be used taken as a percentage of the mean of
     * each sample.
     */
    MINLEN = ".7"
  }
}

/**
 * The following instructs nextflow to collect performance data. You can use
 * this data to generate trace reports. See the Tracing and Visualization section
 * of the Nextflow Documentation for more information.
 *
 * Most likely you do not need to change these settings unless you are familiar
 * with how Nextflow collects trace data and you specifically want to tweak it.
 */
trace {
    enabled = true
    file = 'pipeline_trace.txt'
    fields = 'task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes'
}


/**
 * Nextflow can be executed on any number of high-performance computing
 * infrastructures.  The profile section allows you to customize execution
 * of this workflow for any number of systems.  Please see the Nextflow
 * documentation for a complete list of settings.  Here we provide a few
 * critical settings as well as some examples.
 */
profiles {

  standard {
  /**
   * Indicates the executor plugin. By default this is set to use a local
   * desktop computer.
   */
  process.executor = 'local'

  }

  /**
   * WSU's Kamiak cluster uses the SLURM scheduler. Here we provide
   * an example for execution of this workflow on Kamiak with some
   * defaults for all steps of the workflow
   */
  slurm_cluster {
    process {
      executor = 'slurm'
      queue = {users queue}
      cpus = 1
      time = '4h'
      maxRetries = 2
      errorStrategy = 'retry'
    }
    executor {
      queueSize = 100
    }
  }
}
