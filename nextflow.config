/** 
 * ==================
 * SRA2GEV Paramaters
 * ================== 
 *
 * This file provides the configuration settings needed to customize execution of 
 * this workflow. A description of each setting is provided.
 *
 */
params {

  // The path (full or relative) to the list of SRAs.
  // This must be a file with one SRR number per line.
  sra_list_path = "SRA_IDs.txt"

  // The glob that retrieves locally stored "fastq" files
  //
  // An example of a proper glob to retreive all files with the extension ".fastq" that
  // are contained within sub directories of the imaginary directory "Imagine_Dir_ would be:
  //
  //  "${PWD}/Imagine_Dir/*/*_{1,2}.fastq"
  //
  // If no local files are to be used, set parameter as "none"
  local_samples_path = "/scidas/arabidopsis/all_sra/Kamiak*/[SDE]RR*/*_{1,2}.fastq"

  // Results generated by this workflow are stored in directories that use the 
  // sample name.  Thus, all results for a single sample are contained in its
  // respsective directory.  For SRA files the IDs provided are run IDs (e.g.
  // with SRR, DRR and ERR prefixed numbers), but end-results are stored in folders with
  // their respective experiment IDs (e.g. SRX prefix). The experiment IDs are threfore
  // the sample names.  There are two  parameters so that a file system can be used when there are many fastq
  // files (typically 500 plus files). If you are running this on less than this,
  // we suggest that you use the following parameters:
  //
  //    outputdir_sra = { "${PWD}/${sra}" }
  //    outputdir_srx = { "${PWD}/${srx}" }
  //
  // If you have a large amount of samples, it may be problematic to have hundreds or
  // thousands of sample directories in once place. Therefore, you can use the
  // following organize results into a cascading file system. For example, "SRX0123456" 
  // would have its files in the directory "/SRX/12/34/56/SRR123456/". The following
  // will support this type of output structure.
  //
  //    outputdir_sra = { "${PWD}/${sra[0..2]}/${sra[3..4]}/${sra[5..6]}/${sra.drop(7)}/${sra}" }
  //    outputdir_srx = { "${PWD}/${srx[0..2]}/${srx[3..4]}/${srx[5..6]}/${srx.drop(7)}/${srx}" }
  //
  outputdir_sra = { "${PWD}/${sra}" }
  outputdir_srx = { "${PWD}/${srx}" }


  /**
   * Parameters for the reference genome.
   *
   * The reference genome is provided to this workflow via a set of files housed in a
   * single directory.  The list of files includes:  
   *
   * 1) A FASTA file containing the genomic sequence.
   * 2) Hisat2 index files for the reference.  These index files must
   *    be created using the hisat2-build function.
   * 3) A GTF file containing the genes annotated within hte genome.
   *
   * All files for the reference genome must begin with the same file prefix. For
   * example, if the prefix is TAIR10_Araport11 then the following files should be
   * present all with "TAIR10_Araport11" as the file prefix:
   * TAIR10_Araport11.fna, TAIR10_Araport11.1.ht2, TAIR10_Araport11.2.ht2, (with 
   * potentially more hista2 index files), and TAIR10_Araport11.gtf.
   */
  ref {
  
    // The full file system path of the directory containing the
    // genome reference files.
    path = "/scidas/arabidopsis/reference/"

    // The prefix (used by hisat2-build) for the genome reference files. Note:
    // all files in the reference directory must have this prefix as well.
    prefix = "TAIR10_Araport11"
  }


  /**
   * Parameters for the trimmomatic software.
   *
   * Trimmomatic is used to clean low quality ends and adapters
   * from sequence data.
   */
  trimmomatic {

    // The location of the trimmomatic clipping files.
    clip_path = "/data/ficklin/software/Trimmomatic-0.36/adapters"

    // The minimum read length to be used taken as a percentage
    // of the mean of each sample.
    MINLEN = ".7"
  }
}

/**
 * The following instructs nextflow to collect performance data. You can use
 * this data to generate trace reports. See the Tracing and Visualization section
 * of the Nextflow Documentation for more information.
 *
 * Most likely you do not need to change these settings unless you are familiar 
 * with how Nextflow collects trace data and you specifically want to tweak it.
 */
trace {
    enabled = true
    file = 'pipeline_trace.txt'
    fields = 'task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes'
}


/**
 * Nextflow can be executed on any number of high-performance computing 
 * infrastructures.  The profile section allows you to customize execution
 * of this workflow for any number of systems.  Please see the Nextflow
 * documentation for a complete list of settings.  Here we provide a few
 * critical settings as well as some examples.
 */
profiles {

  standard {
    // Indicates the executor plugin. By default this is set to use
    // a local desktop computer. 
    process.executor = 'local'
  }

  /**
   * WSU's Kamiak cluster uses the SLURM scheduler. Here we provide
   * an example for execution of this workflow on Kamiak with some 
   * defaults for all steps of the workflow
   */
  kamiak {
    process {
      executor = 'slurm'
      queue = 'ficklin'
      cpus = 1
      time = '720h'
      maxRetries = 2
      errorStrategy = 'retry'
    }
    executor {
      queueSize = 100
    }
  }
}
